{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624672ca-9856-46dd-975d-ed19a7f4a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_ = os.environ.setdefault('GEMINI_API_KEY', 'AIzaSyDARA_wlniAc_y4whYoc4w9ibwBFQhzpRo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cfe6f3-26ef-4999-8a11-7003118e3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Runner de interpretabilidad (solo API, sin parsear)\n",
    "# - Lee features desde JSONL generados por el sampler:\n",
    "#     líneas con: {\"sae_label\", \"feature\", \"examples\": [ \"<marked_text>\", ... ]}\n",
    "# - Construye prompts SOLO con ejemplos positivos (sin near-miss)\n",
    "# - Llama a Gemini 2.5 Flash (JSON estricto esperado; aquí no parseamos)\n",
    "# - Guarda resultados en interpretability/data/llm-responses.jsonl con resume idempotente\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from typing import List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "ROOT = Path(\"/Users/josue/proyectos/tesis/low-rank-bilinear-sae\").resolve()\n",
    "INTERP_DIR = ROOT / \"interpretability\"\n",
    "DATA_DIR = INTERP_DIR / \"data\"\n",
    "\n",
    "# Etiquetas SAE (stems de los checkpoints) y K-set\n",
    "SAE_LABELS = [\n",
    "#    'bsae_512_2048_512__Er0_Dr0_k16_ep16_20251129-040858',\n",
    "    'bsae_512_2048_512__Er1_Dr0_k16_ep16_20251129-152503',\n",
    "    'bsae_512_2048_512__Er2_Dr0_k16_ep16_20251129-183808',\n",
    "    'bsae_512_2048_512__Er0_Dr1_k16_ep16_20251129-204654',\n",
    "    'bsae_512_2048_512__Er1_Dr1_k16_ep16_20251129-233024',\n",
    "    'bsae_512_2048_512__Er2_Dr1_k16_ep16_20251130-025537',\n",
    "    'bsae_512_2048_512__Er0_Dr2_k16_ep16_20251130-051124',\n",
    "    'bsae_512_2048_512__Er1_Dr2_k16_ep16_20251130-080037',\n",
    "#    'bsae_512_2048_512__Er2_Dr2_k16_ep16_20251130-113058'\n",
    "]\n",
    "KSET = [16]\n",
    "\n",
    "# Entradas: archivos JSONL con nombre tipo:\n",
    "#   samples_{label}__k{k}__F{...}_E{...}.jsonl\n",
    "SAE_FILES: List[Path] = []\n",
    "for lbl in SAE_LABELS:\n",
    "    for k in KSET:\n",
    "        matches = sorted(DATA_DIR.glob(f\"samples_{lbl}__k{k}__F*_E*.jsonl\"))\n",
    "        if not matches:\n",
    "            raise FileNotFoundError(f\"No se encontró samples para {lbl} k={k} en {DATA_DIR}\")\n",
    "        SAE_FILES.append(matches[-1])  # toma el más “reciente” por orden lexicográfico\n",
    "\n",
    "OUTPUT_JSONL = DATA_DIR / \"llm-responses.jsonl\"\n",
    "\n",
    "GENERATION_MODEL = \"models/gemini-2.5-flash\"\n",
    "\n",
    "# API key SOLO por variable de entorno\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"Falta GEMINI_API_KEY en el entorno. Exporta la variable antes de ejecutar.\")\n",
    "\n",
    "# Límites de cuenta (ajusta si corresponde)\n",
    "LLM_LIMITS = {\"rpm\": 1000, \"tpm\": 1_000_000}\n",
    "MAX_WORKERS = 64\n",
    "\n",
    "# ==============================\n",
    "# I/O jobs y resume\n",
    "# ==============================\n",
    "def _parse_topk_from_stem(stem: str) -> int:\n",
    "    # p.ej., \"samples_...__k16__F1000_E10\" → 16\n",
    "    m = re.search(r\"__k(\\d+)\", stem)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "def load_feature_lines(path: Path, sae_model_label: str, topk_from_file: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Carga líneas JSON (cada una = un feature) en el nuevo formato:\n",
    "      {\"sae_label\": str, \"feature\": int, \"examples\": [ \"<marked_text>\", ... ]}\n",
    "    Devuelve registros normalizados para el runner:\n",
    "      {\"sae_model\", \"topk\", \"feature_id\", \"examples\":[str,...]}\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rec = json.loads(line)\n",
    "            feats.append({\n",
    "                \"sae_model\": sae_model_label,                            # usamos el del archivo\n",
    "                \"topk\": int(rec.get(\"topk\", topk_from_file)),            # desde el nombre del archivo\n",
    "                \"feature_id\": int(rec.get(\"feature\")),                   # nuevo campo\n",
    "                \"examples\": list(rec.get(\"examples\", [])),               # lista de strings marcados\n",
    "            })\n",
    "    return feats\n",
    "\n",
    "def load_all_jobs(file_paths: List[Path]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Lee todos los JSONL y rinde lista de jobs:\n",
    "      {\"sae_model\",\"topk\",\"feature_id\",\"examples\":[str,...]}\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    for fp in tqdm(file_paths, desc=\"Cargando features (JSONL)\"):\n",
    "        if not fp.exists():\n",
    "            raise FileNotFoundError(f\"No existe {fp}\")\n",
    "        stem = fp.stem  # p.ej. \"samples_{lbl}__k16__F1000_E10\"\n",
    "        topk = _parse_topk_from_stem(stem)\n",
    "        # 'sae_model' puro: parte antes de \"__k\"\n",
    "        # stem puede partir con \"samples_{lbl}\", así que extraemos hasta \"__k\"\n",
    "        sae_model_pure = stem.split(\"__k\")[0]\n",
    "        # si empieza con \"samples_\", quítalo para dejar solo el label original\n",
    "        if sae_model_pure.startswith(\"samples_\"):\n",
    "            sae_model_pure = sae_model_pure[len(\"samples_\"):]\n",
    "        feats = load_feature_lines(fp, sae_model_pure, topk)\n",
    "        jobs.extend(feats)\n",
    "    return jobs\n",
    "\n",
    "def load_processed_set(output_path: Path) -> set[Tuple[str, int, int]]:\n",
    "    \"\"\"Set de (sae_model, topk, feature_id) ya escritos en OUTPUT_JSONL.\"\"\"\n",
    "    done: set[Tuple[str, int, int]] = set()\n",
    "    if not output_path.exists():\n",
    "        return done\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            key = (rec.get(\"sae_model\"),\n",
    "                   int(rec.get(\"topk\", -1)),\n",
    "                   int(rec.get(\"feature_id\", -1)))\n",
    "            done.add(key)\n",
    "    return done\n",
    "\n",
    "def filter_pending_jobs(jobs, processed):\n",
    "    return [it for it in jobs if (it[\"sae_model\"], int(it[\"topk\"]), int(it[\"feature_id\"])) not in processed]\n",
    "\n",
    "# ==============================\n",
    "# Rate limiting (RPM/TPM por minuto)\n",
    "# ==============================\n",
    "class RateLimiter:\n",
    "    def __init__(self, rpm: int, tpm: int):\n",
    "        self.rpm = rpm\n",
    "        self.tpm = tpm\n",
    "        self.req_times = []\n",
    "        self.tok_times = []  # (timestamp, tokens)\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    @staticmethod\n",
    "    def approx_tokens(s: str) -> int:\n",
    "        return max(1, len(s) // 4)\n",
    "\n",
    "    def _prune(self, now):\n",
    "        self.req_times = [t for t in self.req_times if now - t < 60]\n",
    "        self.tok_times = [(t, k) for (t, k) in self.tok_times if now - t < 60]\n",
    "\n",
    "    def acquire(self, texts: List[str]):\n",
    "        need = sum(self.approx_tokens(x) for x in texts)\n",
    "        while True:\n",
    "            with self.lock:\n",
    "                now = time.time()\n",
    "                self._prune(now)\n",
    "                used_rpm = len(self.req_times)\n",
    "                used_tpm = sum(k for _, k in self.tok_times)\n",
    "                ok_rpm = (used_rpm + 1) <= self.rpm\n",
    "                ok_tpm = (used_tpm + need) <= self.tpm\n",
    "                if ok_rpm and ok_tpm:\n",
    "                    self.req_times.append(now)\n",
    "                    self.tok_times.append((now, need))\n",
    "                    return\n",
    "                wait = 0.05\n",
    "                if not ok_rpm and self.req_times:\n",
    "                    wait = max(wait, 60 - (now - self.req_times[0]))\n",
    "                if not ok_tpm and self.tok_times:\n",
    "                    wait = max(wait, 60 - (now - self.tok_times[0][0]))\n",
    "            time.sleep(min(2.0, wait))\n",
    "\n",
    "def approx_tokens(s: str) -> int:\n",
    "    return RateLimiter.approx_tokens(s)\n",
    "\n",
    "# ==============================\n",
    "# Normalización de marcadores\n",
    "# ==============================\n",
    "_marker_with_score = re.compile(r\"<<(.*?)>>\\((\\d+(?:\\.\\d+)?)\\)\", re.DOTALL)\n",
    "_marker_no_score   = re.compile(r\"<<(.*?)>>\", re.DOTALL)\n",
    "\n",
    "def _normalize_marker_inner(inner: str) -> str:\n",
    "    s = inner\n",
    "    s = s.replace(\"\\\\n\", \"[NL]\").replace(\"\\n\", \"[NL]\")\n",
    "    s = s.replace(\"\\\\t\", \"[TAB]\").replace(\"\\t\", \"[TAB]\")\n",
    "    if re.fullmatch(r\"[ ]+\", s):\n",
    "        s = \"[WSP]\"\n",
    "    return s\n",
    "\n",
    "def normalize_markers_in_text(text: str) -> str:\n",
    "    def sub_with_score(m: re.Match) -> str:\n",
    "        inner, score = m.group(1), m.group(2)\n",
    "        return f\"<<{_normalize_marker_inner(inner)}>>({score})\"\n",
    "    text2 = _marker_with_score.sub(sub_with_score, text)\n",
    "\n",
    "    def sub_no_score(m: re.Match) -> str:\n",
    "        inner = m.group(1)\n",
    "        return f\"<<{_normalize_marker_inner(inner)}>>\"\n",
    "    text2 = _marker_no_score.sub(sub_no_score, text2)\n",
    "    return text2\n",
    "\n",
    "def needs_whitespace_handling(examples_texts: List[str]) -> bool:\n",
    "    normed = [normalize_markers_in_text(e) for e in examples_texts]\n",
    "    pattern = re.compile(r\"<<(.*?)>>\", re.DOTALL)\n",
    "    for ex in normed:\n",
    "        for inner in pattern.findall(ex):\n",
    "            if any(tok in inner for tok in (\"[NL]\", \"[NL2]\", \"[TAB]\", \"[WSP]\")):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# =========================\n",
    "# Construcción del prompt\n",
    "# =========================\n",
    "def build_prompt_from_record(item: Dict) -> str:\n",
    "    \"\"\"\n",
    "    item (nuevo): {\"sae_model\",\"topk\",\"feature_id\",\"examples\":[str,...]}\n",
    "    NOTA: ya no hay near-miss ni thresholds/deltas.\n",
    "    \"\"\"\n",
    "    k = int(item.get(\"topk\", -1))\n",
    "\n",
    "    # Solo ejemplos positivos (marcados)\n",
    "    pos_texts: List[str] = []\n",
    "    for txt in item.get(\"examples\", []):\n",
    "        pos_texts.append(normalize_markers_in_text(txt))\n",
    "\n",
    "    ws_flag = needs_whitespace_handling(pos_texts)\n",
    "\n",
    "    # Extraer términos “prohibidos” desde marcadores (para evitar simple parroting)\n",
    "    marker_pat = re.compile(r\"<<(.*?)>>(?:\\(\\d+(?:\\.\\d+)?\\))?\", re.DOTALL)\n",
    "    banned: List[str] = []\n",
    "    seen = set()\n",
    "    for t in pos_texts:\n",
    "        for inner in marker_pat.findall(t):\n",
    "            key = inner.strip()\n",
    "            if key and key not in seen:\n",
    "                seen.add(key)\n",
    "                banned.append(key)\n",
    "    if len(banned) > 40:\n",
    "        banned = banned[:40]\n",
    "\n",
    "    def section(title: str, arr: List[str]) -> str:\n",
    "        if not arr:\n",
    "            return f\"{title}:\\n(none)\\n\"\n",
    "        lines = []\n",
    "        for i, t in enumerate(arr, 1):\n",
    "            lines.append(f\"{title} {i}:\\n{t.strip()}\")\n",
    "        return \"\\n\\n\".join(lines) + \"\\n\"\n",
    "\n",
    "    note = \"\"\n",
    "    if ws_flag:\n",
    "        note = (\n",
    "            \"IMPORTANT: If the trigger involves whitespace, do NOT use real newlines or tabs in your output.\\n\"\n",
    "            \"Use placeholders instead: [NL] for newline (\\\\n), [NL2] for two newlines (\\\\n\\\\n), \"\n",
    "            \"[TAB] for tab (\\\\t), [WSP] for a single space. Use them in activating_sentences when needed.\\n\\n\"\n",
    "        )\n",
    "\n",
    "    banned_note = (\n",
    "        \"GUIDANCE:\\n\"\n",
    "        \"- Avoid reusing the exact same trigger tokens/phrases (or close variants) as in the examples.\\n\"\n",
    "        + (\"- In particular, avoid: \" + \", \".join(f'\"{b}\"' for b in banned) + \".\\n\" if banned else \"\")\n",
    "        + \"- You may vary topic, wording, or structure as long as the sentences are likely to activate the feature.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    body = []\n",
    "    body.append(section(\"POSITIVE EXAMPLES (tokens marked; score may be shown)\", pos_texts))\n",
    "    body_text = \"\\n---\\n\\n\".join(body)\n",
    "\n",
    "    return (\n",
    "        \"You are analyzing a single SAE (Sparse Autoencoder) feature in a language model.\\n\"\n",
    "        f\"Top-k context: k = {k}.\\n\"\n",
    "        \"Activated tokens are marked as <<token>>(score). Ignore the markers when writing NEW sentences.\\n\\n\"\n",
    "        f\"{note}\"\n",
    "        \"TASKS:\\n\"\n",
    "        \"1) Under \\\"explanation\\\": Briefly describe the common trigger/pattern.\\n\"\n",
    "        \"2) Under \\\"activating_sentences\\\": Provide EXACTLY 5 NEW sentences that SHOULD strongly activate this feature.\\n\\n\"\n",
    "        f\"{banned_note}\"\n",
    "        \"OUTPUT (STRICT):\\n\"\n",
    "        \"- Return ONLY a valid JSON object with EXACTLY these keys: \\\"explanation\\\", \\\"activating_sentences\\\".\\n\"\n",
    "        \"- \\\"activating_sentences\\\": array of 5 strings.\\n\"\n",
    "        \"- Valid JSON only (double quotes, no trailing commas). No code fences, no comments, no extra text.\\n\\n\"\n",
    "        f\"{body_text}\"\n",
    "        \"OUTPUT EXPECTATION:\\n\"\n",
    "        \"A single valid JSON object with the two keys above and nothing else.\"\n",
    "    )\n",
    "\n",
    "# ==============================\n",
    "# Escritura thread-safe (raw completo)\n",
    "# ==============================\n",
    "_write_lock = threading.Lock()\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    line = json.dumps(obj, ensure_ascii=False) + \"\\n\"\n",
    "    with _write_lock:\n",
    "        with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(line)\n",
    "\n",
    "# ==============================\n",
    "# Cliente + wrapper (sin parsear)\n",
    "# ==============================\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "gemini_model = genai.GenerativeModel(GENERATION_MODEL)\n",
    "\n",
    "class _Retry:\n",
    "    @staticmethod\n",
    "    def parse_retry_delay_seconds(err: Exception) -> int:\n",
    "        m = re.search(r\"retry_delay\\s*\\{\\s*seconds:\\s*(\\d+)\", str(err))\n",
    "        return int(m.group(1)) if m else 0\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, rpm: int, tpm: int):\n",
    "        self.rpm = rpm\n",
    "        self.tpm = tpm\n",
    "        self.req_times = []\n",
    "        self.tok_times = []\n",
    "        self.lock = threading.Lock()\n",
    "    @staticmethod\n",
    "    def approx_tokens(s: str) -> int:\n",
    "        return max(1, len(s) // 4)\n",
    "    def _prune(self, now):\n",
    "        self.req_times = [t for t in self.req_times if now - t < 60]\n",
    "        self.tok_times = [(t, k) for (t, k) in self.tok_times if now - t < 60]\n",
    "    def acquire(self, texts: List[str]):\n",
    "        need = sum(self.approx_tokens(x) for x in texts)\n",
    "        while True:\n",
    "            with self.lock:\n",
    "                now = time.time()\n",
    "                self._prune(now)\n",
    "                used_rpm = len(self.req_times)\n",
    "                used_tpm = sum(k for _, k in self.tok_times)\n",
    "                ok_rpm = (used_rpm + 1) <= self.rpm\n",
    "                ok_tpm = (used_tpm + need) <= self.tpm\n",
    "                if ok_rpm and ok_tpm:\n",
    "                    self.req_times.append(now)\n",
    "                    self.tok_times.append((now, need))\n",
    "                    return\n",
    "                wait = 0.05\n",
    "                if not ok_rpm and self.req_times:\n",
    "                    wait = max(wait, 60 - (now - self.req_times[0]))\n",
    "                if not ok_tpm and self.tok_times:\n",
    "                    wait = max(wait, 60 - (now - self.tok_times[0][0]))\n",
    "            time.sleep(min(2.0, wait))\n",
    "\n",
    "limiter = RateLimiter(rpm=LLM_LIMITS[\"rpm\"], tpm=LLM_LIMITS[\"tpm\"])\n",
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    limiter.acquire([prompt])\n",
    "    try:\n",
    "        resp = gemini_model.generate_content(prompt)\n",
    "        return (resp.text or \"\")\n",
    "    except Exception as e:\n",
    "        rd = _Retry.parse_retry_delay_seconds(e)\n",
    "        time.sleep(rd if rd > 0 else 2.0)\n",
    "        limiter.acquire([prompt])\n",
    "        resp = gemini_model.generate_content(prompt)\n",
    "        return (resp.text or \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e56f445-bdb0-4513-a103-09d5b505ef4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4714a4c78447b1bd01473ff22aa93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cargando features (JSONL):   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] features cargados (total): 3500\n",
      "[INFO] pendientes: 3500 (ya procesados: 1000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6476ffdad594363b0678209abb1d1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM jobs:   0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] escritos=3500 → /Users/josue/proyectos/tesis/low-rank-bilinear-sae/interpretability/data/llm-responses.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Main\n",
    "# ==============================\n",
    "# 1) Cargar jobs\n",
    "jobs = load_all_jobs(SAE_FILES)\n",
    "print(f\"[INFO] features cargados (total): {len(jobs)}\")\n",
    "\n",
    "# 2) Resume\n",
    "done = load_processed_set(OUTPUT_JSONL)\n",
    "pending = filter_pending_jobs(jobs, done)\n",
    "print(f\"[INFO] pendientes: {len(pending)} (ya procesados: {len(done)})\")\n",
    "\n",
    "# 3) Crear archivo de salida si no existe\n",
    "if not OUTPUT_JSONL.exists():\n",
    "    open(OUTPUT_JSONL, \"a\", encoding=\"utf-8\").close()\n",
    "\n",
    "def one_job_runner(item: Dict):\n",
    "    prompt = build_prompt_from_record(item)\n",
    "    raw_text = call_llm(prompt)  # sin parsear (guardamos la respuesta completa)\n",
    "    record = {\n",
    "        \"llm_model\": GENERATION_MODEL,\n",
    "        \"sae_model\": item[\"sae_model\"],       # sin sufijo __k\n",
    "        \"topk\": int(item[\"topk\"]),\n",
    "        \"feature_id\": int(item[\"feature_id\"]),\n",
    "        # Nuevo formato: solo ejemplos positivos marcados\n",
    "        \"examples\": item[\"examples\"],         # eco para auditoría\n",
    "        # Respuesta cruda del LLM (debe contener \"explanation\" y \"activating_sentences\")\n",
    "        \"raw\": raw_text,\n",
    "        \"approx_prompt_tokens\": approx_tokens(prompt),  # auditoría\n",
    "        \"expected_output_keys\": [\"explanation\", \"activating_sentences\"],\n",
    "    }\n",
    "    append_jsonl(OUTPUT_JSONL, record)\n",
    "    return 1\n",
    "\n",
    "total_done = 0\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
    "    futures = [pool.submit(one_job_runner, it) for it in pending]\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"LLM jobs\"):\n",
    "        try:\n",
    "            total_done += fut.result()\n",
    "        except Exception as e:\n",
    "            print(\"[JOB ERROR]\", e)\n",
    "\n",
    "print(f\"[DONE] escritos={total_done} → {OUTPUT_JSONL.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
