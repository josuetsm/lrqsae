{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63afc75e-31f5-44ba-bf1f-5d8fa7f8fc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bsae_512_2048_512__Er1_Dr0_k16_ep16_20251129-152503] Cargando SAE / estados…\n",
      "[bsae_512_2048_512__Er1_Dr0_k16_ep16_20251129-152503] Muestreando 500 features; 10 ejemplos por feature…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510bbb30b85242d89a5cbd758ab1e4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[bsae_512_2048_512__Er1_Dr0_k16_ep16_20251129-152503] features:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bsae_512_2048_512__Er1_Dr0_k16_ep16_20251129-152503: 500 features escritos → /Users/josue/proyectos/tesis/low-rank-bilinear-sae/interpretability/data/samples_bsae_512_2048_512__Er1_Dr0_k16_ep16_20251129-152503__k16__F500_E10.jsonl\n",
      "[bsae_512_2048_512__Er2_Dr0_k16_ep16_20251129-183808] Cargando SAE / estados…\n",
      "[bsae_512_2048_512__Er2_Dr0_k16_ep16_20251129-183808] Muestreando 500 features; 10 ejemplos por feature…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc2236c410348c48709da3eb58c2a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[bsae_512_2048_512__Er2_Dr0_k16_ep16_20251129-183808] features:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bsae_512_2048_512__Er2_Dr0_k16_ep16_20251129-183808: 500 features escritos → /Users/josue/proyectos/tesis/low-rank-bilinear-sae/interpretability/data/samples_bsae_512_2048_512__Er2_Dr0_k16_ep16_20251129-183808__k16__F500_E10.jsonl\n",
      "[bsae_512_2048_512__Er0_Dr1_k16_ep16_20251129-204654] Cargando SAE / estados…\n",
      "[bsae_512_2048_512__Er0_Dr1_k16_ep16_20251129-204654] Muestreando 500 features; 10 ejemplos por feature…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f55b481b8f649cc8d5d152ee9d3aaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[bsae_512_2048_512__Er0_Dr1_k16_ep16_20251129-204654] features:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bsae_512_2048_512__Er0_Dr1_k16_ep16_20251129-204654: 500 features escritos → /Users/josue/proyectos/tesis/low-rank-bilinear-sae/interpretability/data/samples_bsae_512_2048_512__Er0_Dr1_k16_ep16_20251129-204654__k16__F500_E10.jsonl\n",
      "[bsae_512_2048_512__Er1_Dr1_k16_ep16_20251129-233024] Cargando SAE / estados…\n",
      "[bsae_512_2048_512__Er1_Dr1_k16_ep16_20251129-233024] Muestreando 500 features; 10 ejemplos por feature…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150d32c9c4d647d796f4eaad880f5157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[bsae_512_2048_512__Er1_Dr1_k16_ep16_20251129-233024] features:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bsae_512_2048_512__Er1_Dr1_k16_ep16_20251129-233024: 500 features escritos → /Users/josue/proyectos/tesis/low-rank-bilinear-sae/interpretability/data/samples_bsae_512_2048_512__Er1_Dr1_k16_ep16_20251129-233024__k16__F500_E10.jsonl\n",
      "[bsae_512_2048_512__Er2_Dr1_k16_ep16_20251130-025537] Cargando SAE / estados…\n",
      "[bsae_512_2048_512__Er2_Dr1_k16_ep16_20251130-025537] Muestreando 500 features; 10 ejemplos por feature…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7f7497240d4192aaefbe4a3371cc34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[bsae_512_2048_512__Er2_Dr1_k16_ep16_20251130-025537] features:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bsae_512_2048_512__Er2_Dr1_k16_ep16_20251130-025537: 500 features escritos → /Users/josue/proyectos/tesis/low-rank-bilinear-sae/interpretability/data/samples_bsae_512_2048_512__Er2_Dr1_k16_ep16_20251130-025537__k16__F500_E10.jsonl\n",
      "[bsae_512_2048_512__Er0_Dr2_k16_ep16_20251130-051124] Cargando SAE / estados…\n",
      "[bsae_512_2048_512__Er0_Dr2_k16_ep16_20251130-051124] Muestreando 500 features; 10 ejemplos por feature…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797c29a53b62436baf6c4fd7eed58fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[bsae_512_2048_512__Er0_Dr2_k16_ep16_20251130-051124] features:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bsae_512_2048_512__Er0_Dr2_k16_ep16_20251130-051124: 500 features escritos → /Users/josue/proyectos/tesis/low-rank-bilinear-sae/interpretability/data/samples_bsae_512_2048_512__Er0_Dr2_k16_ep16_20251130-051124__k16__F500_E10.jsonl\n",
      "[bsae_512_2048_512__Er1_Dr2_k16_ep16_20251130-080037] Cargando SAE / estados…\n",
      "[bsae_512_2048_512__Er1_Dr2_k16_ep16_20251130-080037] Muestreando 500 features; 10 ejemplos por feature…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e346a3b21144cead9693f690c5c4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[bsae_512_2048_512__Er1_Dr2_k16_ep16_20251130-080037] features:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] bsae_512_2048_512__Er1_Dr2_k16_ep16_20251130-080037: 500 features escritos → /Users/josue/proyectos/tesis/low-rank-bilinear-sae/interpretability/data/samples_bsae_512_2048_512__Er1_Dr2_k16_ep16_20251130-080037__k16__F500_E10.jsonl\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Muestreo de 1000 features por SAE y 10 ejemplos por feature (secuencias distintas),\n",
    "marcando TODOS los tokens de la secuencia donde z_f >= tau_k, usando token_ids desde JSONL.\n",
    "\n",
    "Salida por SAE (JSONL):\n",
    "- interpretability/data/samples_{label}__k{k}__F1000_E10.jsonl\n",
    "  con registros: { \"sae_label\", \"feature\", \"examples\" }\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import zarr\n",
    "import mlx.core as mx\n",
    "from tqdm.notebook import tqdm\n",
    "from mlx_lm.utils import load_tokenizer\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "ROOT = Path(\"/Users/josue/proyectos/tesis/low-rank-bilinear-sae\").resolve()\n",
    "INTERP_DIR = ROOT / \"interpretability\"\n",
    "DATA_DIR = INTERP_DIR / \"data\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Zarr con activaciones del residual stream (inputs del SAE)\n",
    "ZARR_ACT_PATH = \"/Users/josue/proyectos/tesis/pythia/data/Pythia70M-L3-res-wiki.zarr\"\n",
    "\n",
    "# JSONL con listas de token ids (cada línea es una lista de enteros)\n",
    "TOKEN_IDS_JSONL = \"/Users/josue/proyectos/tesis/pythia/data/Pythia70M-L3-res-wiki-token-ids.jsonl\"\n",
    "\n",
    "# Tokenizer (ruta local o nombre en HF compatible con mlx_lm.utils)\n",
    "TOKENIZER_PATH = Path(\"EleutherAI/pythia-70m-deduped\")\n",
    "\n",
    "# SAEs a procesar\n",
    "SAE_PATHS = [\n",
    "#    'checkpoints/bsae_512_2048_512__Er0_Dr0_k16_ep16_20251129-040858.pkl',\n",
    "    'checkpoints/bsae_512_2048_512__Er1_Dr0_k16_ep16_20251129-152503.pkl',\n",
    "    'checkpoints/bsae_512_2048_512__Er2_Dr0_k16_ep16_20251129-183808.pkl',\n",
    "    'checkpoints/bsae_512_2048_512__Er0_Dr1_k16_ep16_20251129-204654.pkl',\n",
    "    'checkpoints/bsae_512_2048_512__Er1_Dr1_k16_ep16_20251129-233024.pkl',\n",
    "    'checkpoints/bsae_512_2048_512__Er2_Dr1_k16_ep16_20251130-025537.pkl',\n",
    "    'checkpoints/bsae_512_2048_512__Er0_Dr2_k16_ep16_20251130-051124.pkl',\n",
    "    'checkpoints/bsae_512_2048_512__Er1_Dr2_k16_ep16_20251130-080037.pkl',\n",
    "#    'checkpoints/bsae_512_2048_512__Er2_Dr2_k16_ep16_20251130-113058.pkl'\n",
    "]\n",
    "SAE_LABELS = [Path(p).stem for p in SAE_PATHS]\n",
    "\n",
    "# Parámetros de muestreo\n",
    "K = 16                # τ_k y elegibilidad\n",
    "N_FEATURES = 500     # nº de features por SAE\n",
    "N_EXAMPLES = 10       # nº de ejemplos (secuencias únicas) por feature\n",
    "GLOBAL_RNG_SEED = 20251127\n",
    "\n",
    "# ---------- UTILIDADES ----------\n",
    "from model import *  # Debe exponer .load(), .encoder(), .clip_norms()\n",
    "\n",
    "def detect_F_total(sae) -> int:\n",
    "    try:\n",
    "        if hasattr(sae, \"decoder\"):\n",
    "            dec = sae.decoder\n",
    "            if hasattr(dec, \"weight\"):\n",
    "                return int(dec.weight.shape[1])\n",
    "            if hasattr(dec, \"W\"):\n",
    "                return int(dec.W.shape[1])\n",
    "        if hasattr(sae, \"F\"):\n",
    "            return int(sae.F)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 2048\n",
    "\n",
    "def search_seq_id(cumsum: np.ndarray, gidx: int) -> int:\n",
    "    return int(np.searchsorted(cumsum, int(gidx), side=\"right\"))\n",
    "\n",
    "# Usa tokenizer global para decodificar; marcamos tokens con z_f >= thr\n",
    "def mark_sequence(seq_token_ids: np.ndarray, z_feat_seq: np.ndarray, thr: float) -> str:\n",
    "    out = []\n",
    "    thr = float(thr)\n",
    "    for tid, s in zip(seq_token_ids, z_feat_seq):\n",
    "        tok = tokenizer.decode(int(tid))\n",
    "        if float(s) >= thr:\n",
    "            out.append(f\"<<{tok}>>({float(s):.2f})\")\n",
    "        else:\n",
    "            out.append(tok)\n",
    "    return \"\".join(out)\n",
    "\n",
    "# ---------- CARGA ARRAYS Y TOKEN IDS ----------\n",
    "act_zarr = zarr.open(ZARR_ACT_PATH, mode=\"r\")   # [N, D]\n",
    "TOTAL_TOKENS = int(act_zarr.shape[0])\n",
    "\n",
    "# Cargar token_ids_docs JSONL y construir cumsum alineado a TOTAL_TOKENS (idéntico a tu procedimiento)\n",
    "with open(TOKEN_IDS_JSONL, \"r\") as f:\n",
    "    token_ids_docs = [json.loads(line) for line in f]\n",
    "\n",
    "token_ids_cumsum = np.cumsum([len(doc) for doc in token_ids_docs], dtype=np.int64)\n",
    "\n",
    "# Recorta/ajusta el último documento para alinear con TOTAL_TOKENS\n",
    "last_seq = np.abs(TOTAL_TOKENS - token_ids_cumsum).argmin().item()\n",
    "token_ids_docs = token_ids_docs[:last_seq + 1]\n",
    "token_ids_docs[last_seq] = token_ids_docs[last_seq][:-int(token_ids_cumsum[last_seq] - TOTAL_TOKENS)]\n",
    "token_ids_cumsum = np.cumsum([len(doc) for doc in token_ids_docs], dtype=np.int64)\n",
    "\n",
    "#assert token_ids_cumsum[-1] == TOTAL_TOKENS, \"Desalineación: TOTAL_TOKENS != suma de token_ids_docs\"\n",
    "\n",
    "tokenizer = load_tokenizer(TOKENIZER_PATH)\n",
    "rng = np.random.default_rng(GLOBAL_RNG_SEED)\n",
    "\n",
    "# ---------- LOOP POR SAE ----------\n",
    "for sae_path, label in zip(SAE_PATHS, SAE_LABELS):\n",
    "    print(f\"[{label}] Cargando SAE / estados…\")\n",
    "    sae = BilinearSparseAutoencoder.load(sae_path)\n",
    "    F_TOTAL = detect_F_total(sae)\n",
    "\n",
    "    # τ_k desde scan_state\n",
    "    scan_pkl = DATA_DIR / f\"scan_state_{label}.pkl\"\n",
    "    with open(scan_pkl, \"rb\") as pf:\n",
    "        scan_state = pickle.load(pf)\n",
    "    if int(K) not in scan_state[\"taus\"]:\n",
    "        raise ValueError(f\"[{label}] τ_k ausente en scan_state para k={K}.\")\n",
    "    tau_k = float(scan_state[\"taus\"][int(K)])\n",
    "\n",
    "    # reservoirs\n",
    "    res_pkl = DATA_DIR / f\"reservoirs_{label}.pkl\"\n",
    "    with open(res_pkl, \"rb\") as pf:\n",
    "        reservoirs = pickle.load(pf)\n",
    "    res_k = reservoirs[\"reservoir\"][int(K)]\n",
    "    gidx_lists = res_k[\"gidx\"]   # list[F] de listas de gidx\n",
    "    assert len(gidx_lists) == F_TOTAL, \"Reservoir no coincide con F_TOTAL.\"\n",
    "\n",
    "    # Elegibles: features con ≥1 gidx en reservoir\n",
    "    eligible_feats = [f for f in range(F_TOTAL) if len(gidx_lists[f]) > 0]\n",
    "#    if len(eligible_feats) == 0:\n",
    "#        raise RuntimeError(f\"[{label}] Sin features elegibles en reservoir para k={K}.\")\n",
    "#    if len(eligible_feats) < N_FEATURES:\n",
    "#        print(f\"[{label}] Aviso: solo {len(eligible_feats)} features con elegibles; se usan todos.\")\n",
    "#        chosen_feats = np.array(eligible_feats, dtype=np.int64)\n",
    "#    else:\n",
    "    chosen_feats = rng.choice(2048, size=N_FEATURES, replace=False,\n",
    "                              p=np.array(scan_state['pos_counts_all'][int(K)]) / sum(scan_state['pos_counts_all'][int(K)]))\n",
    "\n",
    "    out_jsonl = DATA_DIR / f\"samples_{label}__k{K}__F{len(chosen_feats)}_E{N_EXAMPLES}.jsonl\"\n",
    "    print(f\"[{label}] Muestreando {len(chosen_feats)} features; {N_EXAMPLES} ejemplos por feature…\")\n",
    "\n",
    "    n_feats_written = 0\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as jf, tqdm(total=len(chosen_feats), desc=f\"[{label}] features\") as pbar:\n",
    "        for fidx in chosen_feats:\n",
    "            fidx = int(fidx)\n",
    "            cand_g = np.array(gidx_lists[fidx], dtype=np.int64)\n",
    "            if cand_g.size == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            rng.shuffle(cand_g)\n",
    "\n",
    "            # Recolectar ejemplos (secuencias únicas) con activación real del feature\n",
    "            examples = []\n",
    "            seen_seq = set()\n",
    "\n",
    "            for g in cand_g:\n",
    "                sid = search_seq_id(token_ids_cumsum, int(g))\n",
    "                if sid in seen_seq:\n",
    "                    continue\n",
    "\n",
    "                # Delimitar secuencia\n",
    "                seq_end = int(token_ids_cumsum[sid])\n",
    "                seq_start = int(token_ids_cumsum[sid - 1]) if sid > 0 else 0\n",
    "                if not (0 <= seq_start < seq_end <= TOTAL_TOKENS):\n",
    "                    continue\n",
    "\n",
    "                # Activaciones y tokens de la secuencia\n",
    "                x_seq = mx.array(act_zarr[seq_start:seq_end], dtype=mx.float32)   # [L, D]\n",
    "                tok_seq = np.array(token_ids_docs[sid], dtype=np.int64)           # [L]\n",
    "                L = seq_end - seq_start\n",
    "                if len(tok_seq) != L:\n",
    "                    tok_seq = tok_seq[:L]  # seguridad\n",
    "\n",
    "                # Codificar y extraer columna del feature (→ NumPy para evitar indexación MLX)\n",
    "                z_full = sae.encoder(sae.clip_norms(x_seq))                       # [L, F] (MLX)\n",
    "                z_full_np = np.array(z_full, dtype=np.float32)                    # [L, F] (NumPy)\n",
    "                z_feat_seq = z_full_np[:, fidx]                                   # [L]\n",
    "\n",
    "                # Requisito: debe haber activación del feature en la secuencia\n",
    "                if not np.any(z_feat_seq >= tau_k):\n",
    "                    continue\n",
    "\n",
    "                # Registrar ejemplo (texto marcado)\n",
    "                marked_text = mark_sequence(tok_seq, z_feat_seq, thr=tau_k)\n",
    "                examples.append(marked_text)\n",
    "                seen_seq.add(sid)\n",
    "\n",
    "                if len(examples) >= N_EXAMPLES:\n",
    "                    break\n",
    "\n",
    "            # Emitir registro solo si hay ≥1 ejemplo\n",
    "            if len(examples) > 0:\n",
    "                rec = {\n",
    "                    \"sae_label\": label,\n",
    "                    \"feature\": int(fidx),\n",
    "                    \"examples\": examples,\n",
    "                }\n",
    "                jf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "                n_feats_written += 1\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"[OK] {label}: {n_feats_written} features escritos → {out_jsonl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e704c-8940-4fb8-acc4-ad7598630770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698a620-8c93-484d-8ee2-a3bd9f5264cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
